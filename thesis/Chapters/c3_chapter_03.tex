%!TEX root = ../thesis.tex
\chapter{Попередня обробка даних, побудова моделей та оцінка методів}
\label{chap:practice}

В даному розділі описано попередню обробку даних, які використовувалися в дослідженні, та описано параметри побудованих моделей. Також описано процес навчання та підбору гіперпараметрів моделей. Вказана перевірка моделей на тестових даних, за допомогою метрик: accuracy, precision, recall та f1-score.

\section{Попередня обробка даних}

Як вже зазначалось в даній роботі використовується три набори даних, а саме: Pima Indians Diabetes Database~\cite{ct30}, Human Activity Recognition with Smartphones~\cite{ct31} та Chest X-Ray Images (Pneumonia)~\cite{ct32}. Для кожного набору даних була застосована своя попередня обробка. Далі ми наведемо, які методи обробки були застосовані до кожного набору.

Почнемо розгляд з Pima Indians Diabetes Database (прочитати детальніше, про датасет можна у~\cite{ct30} або у розділі \ref{sec:data-description}). Першим кроком попередньої обробки була заміна нульових значень, які зустрічаються у деяких змінних (Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age) на відсутні значення (nan). Це дозволяє уникнути впливу невірних даних на подальший аналіз. Далі, для кожної змінної, у якої були відсутні значення, було обчислено медіанні значення для груп з позитивним та негативним результатом по діабету (Outcome). Відсутні значення заповнювалися відповідно до медіанної величини для відповідної групи. Після цього було проведено обробку змінної Insulin для видалення викидів. Викиди визначалися за допомогою методу Interquartile Range Technique~\cite{ct33}. Наступним кроком було виявлення та видалення викидів за допомогою методу Local Outlier Factor~\cite{ct34}. Цей метод використовує локальну щільність сусідів для визначення аномалій. Після розрахунку негативного фактору аномалії для кожного зразка, було визначено порогове значення, і видалено ті зразки, які мали значення нижче цього порогу. Після видалення аномалій дані були розділені на ознаки та мітки. Вибірки було поділено на тренувальний та тестовий набори даних у пропорції 80:20. Потім, для тренувального та тестового наборів даних було проведено стандартизацію ознак шляхом видалення середнього значення та масштабування до одиничної дисперсії. В результаті ми отримали оброблений датасет, який будемо використовувати для порівняння моделей для задачі бінарної класифікації табличних даних.

Наступну попередню обробку опишемо для набору даних Human Activity Recognition with Smartphones (прочитати детальніше, про датасет можна у~\cite{ct31} або у розділі \ref{sec:data-description}). Для цього набору даних було застосовано наступні методи попередньої обробки. По-перше, дані було розділено на тренувальну та тестову вибірки, кожна з яких містила відповідні дані для тренування та тестування моделей. Після завантаження датасетів для тренування та тестування, було застосовано LabelEncoder для кодування міток активностей (Activity) у числовий формат. Наступним кроком було розділення даних на ознаки та мітки для тренувальних і тестових вибірок. Далі було проведено випадкове перемішування тренувальних та тестових даних для уникнення впливу можливого порядку даних на результати навчання моделей. Для покращення роботи моделей було проведено стандартизацію ознак шляхом видалення середнього значення та масштабування до одиничної дисперсії. Цей процес дозволяє моделі краще адаптуватися до даних, які мають різний масштаб. В результаті попередньої обробки ми отримали стандартизовані та перемішані тренувальні та тестові набори даних, готові до подальшого використання у моделюванні. 

Нарешті, розглянемо попередню обробку даних для набору даних Chest X-Ray Images (Pneumonia) (прочитати детальніше про датасет можна у~\cite{ct32} або у розділі \ref{sec:data-description}). Для цього набору даних було застосовано наступні методи попередньої обробки. По-перше, дані були завантажені та організовані у вигляді класів, де кожне зображення має відповідну мітку (0 - нормальний, 1 - бактеріальна пневмонія, 2 - вірусна пневмонія). Як зазначалося ми використали цей датасет для бінарної та багатокласової класифікації, для бінарної, відповідно, класи бактеріальна та вірусна пневмонії були об'єднані в один клас -- пневмонія, а для багатокласової використовувалися класи нормальний, бактеріальна пневмонія та вірусна пневмонія. Зображення були перетворені до розміру 224x224 пікселів і конвертовані в градації сірого для уніфікації формату. Для екстракції ознак було використано попередньо натреновану модель ResNet-50~\cite{ct35} без останнього повнозв'язного шару. Модель була завантажена з збережених ваг (які ми самі натренували використовуючи цей же набір даних) та використана для отримання векторів ознак зображень. Далі, для обробки отриманих векторів ознак, було застосовано стандартизацію ознак шляхом видалення середнього значення та масштабування до одиничної дисперсії. Для зменшення розмірності та збереження 99$\%$ варіативності даних було застосовано метод Principal Component Analysis~\cite{ct36}. В результаті ми отримали оброблені вектори ознак, готові до подальшого використання у моделюванні.

\section{Детальний опис моделей та підбір гіперпараметрів моделей}

Як вже було зазначено в даній роботі ми сфокусувалися на тестуванні моделей MLP with gradient descent, MLP with single-point mutation та $(1+\lambda)$-EA with GP encodings. Принцип тренування MLP with gradient descent ми описали в розділі \ref{sec:training_process}, тому в цьому розділі ми опишемо детально процес навчання моделей MLP with single-point mutation та $(1+\lambda)$-EA with GP encodings.

MLP with single-point mutation використовує оптимізаційний алгоритм, який працює наступним чином: на кожній епосі випадковим чином обирається значення однієї ваги з усієї нейронної мережі та до нього додається значення випадкової величини, яка має нормальний розподіл з нульовим математичним сподіванням та певним значенням дисперсії, після цього розраховується функція втрат з оновленими вагами, якщо її значення стало меньше, ніж було з початковими вагами на поточній епосі, то ваги зберігаються і процес переходить на наступну епоху, якщо функція втрат збільшилася, то повертаються ваги, які були до додавання значення випадкової величини і відбувається перехід на наступну епоху. Цей процес повторюється задану кількість епох.

Метод $(1+\lambda)$-EA with GP encodings описується наступним чином. Першим кроком ініціалізується індивід, в нашому випадку індивідом є дерево у якого в якості внутрішніх вузлів -- функції, які мають арність 2, а в якості листків -- features датасету або константи з набору: $1, 0, -1, e, \pi$. Для цього індивіду розраховується фітнес-функція. Після того, як була розрахована фітнес-функція для створеного індивіду, він мутує $\lambda$ разів, таким чином ми отримуємо $\lambda$ нових індивідів. Мутація в даному випадку відбувається наступним чином: випадково обирається один вузол з усього індивіду і його значення замінюється на якесь інше випадкове, валідне значення (у випадку внутрішніх вузлів -- значення замінюється на якусь іншу функцію, а у випадку листків на якусь іншу feature, або константу).  Після цього ми розраховуємо фітнес-функції для усіх новостворених індивідів і обираємо індивід, який має найменше значення фітнес-функцї. Цей індивід далі виступає в якості батька на наступних ітераціях.

Тепер після того, як ми описали, як в нашому випадку працюють алгоритми, перейдемо до процесу підбору оптимальних гіперпараметрів. Процес вибору гіперпараметрів є важливим кроком в тренуванні моделей, оскільки від них значно залежить швидкість конвергенції та якість моделей. Тому в цій частині ми опишемо, процес за яким відбувається підбір гіперпараметрів для різних задач та моделей, а також які саме гіперпараметри виявилися найкращими і які ми використовуємо.

Почнемо розгляд з задач класифікації та моделі MLP with gradient descent. Для підбору гіперпараметрів для цієї моделі ми використовували бібліотеку optuna~\cite{ct22}, яка дозволяє проводити байєсівську оптимізацію~\cite{ct37}. Для цього ми задали простір гіперпараметрів по якому проводився пошук оптимальних з них за 1000 ітерацій. Найкращі гіперпараметри для кожної задачі можна подивитися у таблиці \ref{tab_hyperparameters_for_mlp_with_gd}.

\begin{table}[ht]
	\caption{Найкращі гіперпараметри для моделі MLP with gradient descent}
	\label{tab_hyperparameters_for_mlp_with_gd}
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
			\hline \multirow{2}{*}{Гіперпараметри} & \multicolumn{4}{c|}{Задачі} \\
			\cline{2-5} & Бінарна класифікація табличних даних & Бінарна класифікація картинок & Багатокласова класифікація табличних даних & Багатокласова класифікація картинок \\
			\hline hidden\_layer\_sizes & (10, 15, 10) & (15, 20, 15) & (10, 10) & (10, 10) \\
			\hline activation & tanh & tanh & logistic & tanh \\
			\hline solver & sgd & sgd & sgd & sgd \\
			\hline alpha & 0.0009 & 0.0054 & 0.0001 & 0.0001 \\
			\hline learning\_rate\_init & 0.004 & 0.002 & 0.008 & 0.001 \\
			\hline learning\_rate & adaptive & invscaling & adaptive & invscaling \\
			\hline batch\_size & 32 & 256 & 64 & 128 \\
			\hline tol & 0.00002 & 0.00033 & 0.00023 & 0.00003 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

Для моделі MLP with single-point mutation для пошуку оптимальних гіперпараметрів також було застосовану байєсівську оптимізацію~\cite{ct37} за 1000 ітерацій. В результаті ми отримали оптимальні гіперпараметри, які можна подивитися у таблиці \ref{tab_hyperparameters_for_mlp_with_sp_mut}.

\begin{table}[ht]
	\caption{Найкращі гіперпараметри для моделі MLP with single-point mutation}
	\label{tab_hyperparameters_for_mlp_with_sp_mut}
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
			\hline \multirow{2}{*}{Гіперпараметри} & \multicolumn{4}{c|}{Задачі} \\
			\cline{2-5} & Бінарна класифікація табличних даних & Бінарна класифікація картинок & Багатокласова класифікація табличних даних & Багатокласова класифікація картинок \\
			\hline hidden\_layer\_sizes & (10, 15, 20, 15, 10) & (15, 20, 15) & () & (10, 10) \\
			\hline scale\_for\_mutation & 0.5 & 0.1 & 0.1 & 0.1 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

Останньою моделлю для якої ми шукали оптимальні параметри є $(1+\lambda)$-EA with GP encoding. Процес підбору гіпепараметрів для цієї моделі відрізнявся від процесу для двох вище згаданих моделей

\chapconclude{\ref{chap:practice}}

Висновки до останнього розділу є, фактично, підсумковими під усім 
дослідженням; однак вони повинні стостуватись саме того, що розглядалось у 
розділі.